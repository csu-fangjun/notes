<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Internals &mdash; notes  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="torch.quantize_per_tensor" href="quantize_per_tensor.html" />
    <link rel="prev" title="Quantization" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> notes
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../cpp/index.html">C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sphinx/index.html">Sphinx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../git/index.html">git</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docker/index.html">docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../latex/index.html">LaTeX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../kaldi/index.html">Kaldi</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bash/index.html">bash</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cuda/index.html">CUDA</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">torch</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../load-and-save.html">torch.load and torch.save</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gather.html">torch.gather</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ddp/index.html">DDP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../torchscript/index.html">TorchScript</a></li>
<li class="toctree-l2"><a class="reference internal" href="../logical-op.html">Logical operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notes.html">Note</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Quantization</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Internals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#qscheme">QScheme</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pertensoraffinequantizer">PerTensorAffineQuantizer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="quantize_per_tensor.html">torch.quantize_per_tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="quantize_per_tensor_dynamic.html">quantize_per_tensor_dynamic</a></li>
<li class="toctree-l3"><a class="reference internal" href="quantize_per_channel.html">torch.quantize_per_channel</a></li>
<li class="toctree-l3"><a class="reference internal" href="observer.html">Observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="hello.html">Hello</a></li>
<li class="toctree-l3"><a class="reference internal" href="references.html">References</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../android/index.html">android</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/index.html">onnx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lstm.html">nn.LSTM</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../python/index.html">Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../java/index.html">java</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../javascript/index.html">javascript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../html/index.html">HTML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../css/index.html">css</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pybind11/index.html">pybind11</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../protobuf/index.html">Protocol Buffers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../grpc/index.html">gRPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../lwn_net/index.html">lwn.net</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../linker_and_loader/index.html">Linker and Loader</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../espnet/index.html">espnet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cmake/index.html">cmake</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../huggingface/index.html">huggingface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../e6870/index.html">EECS E6870 Speech Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ncnn/index.html">ncnn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../llvm/index.html">LLVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../android/index.html">Android</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../qemu/index.html">qemu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sox/index.html">sox</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mnn/index.html">MNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../simd/index.html">SIMD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../asio/index.html">asio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../websocketpp/index.html">websocketpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../os/index.html">Operating systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../encoding/index.html">encoding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ios/index.html">ios</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../embedded_systems/index.html">Embedded systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ssh/index.html">ssh</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onnx/index.html">onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../csharp/index.html">csharp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../flask/index.html">Flask</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../arm/index.html">ARM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../vitualbox/index.html">VirtualBox</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../go/index.html">Go</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../whisper/index.html">Whisper</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../windows/index.html">Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../qt/index.html">qt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../webassembly/index.html">webassembly</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../spleeter/index.html">spleeter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../django/index.html">django</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../react/index.html">React</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">notes</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../index.html">torch</a> &raquo;</li>
          <li><a href="index.html">Quantization</a> &raquo;</li>
      <li>Internals</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/csu-fangjun/notes/blob/master/docs/source/torch/quantization/internals.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="internals">
<h1>Internals<a class="headerlink" href="#internals" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/core/QuantizerBase.h">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/core/QuantizerBase.h</a>
defines the base class <code class="docutils literal notranslate"><span class="pre">Quantizer</span></code>.</p>
<p><a class="reference external" href="https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/quantized/Quantizer.h">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/quantized/Quantizer.h</a>
defines the subclasses of <code class="docutils literal notranslate"><span class="pre">Quantizer</span></code>, such as</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">PerTensorAffineQuantizer</span></code> - qscheme is <code class="docutils literal notranslate"><span class="pre">kPerTensorAffine</span></code>.</p></li>
</ul>
</div></blockquote>
<section id="qscheme">
<h2>QScheme<a class="headerlink" href="#qscheme" title="Permalink to this headline">¶</a></h2>
<p>See <a class="reference external" href="https://github.com/pytorch/pytorch/blob/master/c10/core/QScheme.h">https://github.com/pytorch/pytorch/blob/master/c10/core/QScheme.h</a></p>
<div class="literal-block-wrapper docutils container" id="id2">
<div class="code-block-caption"><span class="caption-text">./code/qscheme/main.cc</span><a class="headerlink" href="#id2" title="Permalink to this code">¶</a></div>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;torch/script.h&quot;</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="k">static</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="nf">TestQScheme</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="linenos"> 4</span><span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">toString</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kPerTensorAffine</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;per_tensor_affine&quot;</span><span class="p">);</span>
<span class="linenos"> 5</span>
<span class="linenos"> 6</span><span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">toString</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kPerChannelAffine</span><span class="p">)</span><span class="w"> </span><span class="o">==</span>
<span class="linenos"> 7</span><span class="w">              </span><span class="s">&quot;per_channel_affine&quot;</span><span class="p">);</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">toString</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kPerTensorSymmetric</span><span class="p">)</span><span class="w"> </span><span class="o">==</span>
<span class="linenos">10</span><span class="w">              </span><span class="s">&quot;per_tensor_symmetric&quot;</span><span class="p">);</span>
<span class="linenos">11</span>
<span class="linenos">12</span><span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">toString</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kPerChannelSymmetric</span><span class="p">)</span><span class="w"> </span><span class="o">==</span>
<span class="linenos">13</span><span class="w">              </span><span class="s">&quot;per_channel_symmetric&quot;</span><span class="p">);</span>
<span class="linenos">14</span>
<span class="linenos">15</span><span class="w">  </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">toString</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kPerChannelAffineFloatQParams</span><span class="p">)</span><span class="w"> </span><span class="o">==</span>
<span class="linenos">16</span><span class="w">              </span><span class="s">&quot;per_channel_affine_float_qparams&quot;</span><span class="p">);</span>
<span class="linenos">17</span><span class="p">}</span>
<span class="linenos">18</span>
<span class="linenos">19</span><span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="linenos">20</span><span class="w">  </span><span class="n">TestQScheme</span><span class="p">();</span>
<span class="linenos">21</span><span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="linenos">22</span><span class="p">}</span>
</pre></div>
</div>
</div>
</section>
<section id="pertensoraffinequantizer">
<h2>PerTensorAffineQuantizer<a class="headerlink" href="#pertensoraffinequantizer" title="Permalink to this headline">¶</a></h2>
<p>It has 4 important methods:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">QScheme</span> <span class="pre">qscheme()</span> <span class="pre">const</span></code>, always returns <code class="docutils literal notranslate"><span class="pre">kPerTensorAffine</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">double</span> <span class="pre">scale()</span> <span class="pre">const</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">int64_t</span> <span class="pre">zero_point()</span> <span class="pre">const</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ScalarType</span> <span class="pre">scalar_type()</span> <span class="pre">const</span></code></p></li>
</ul>
</div></blockquote>
<p>It uses <a class="reference external" href="https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp#L3199">quantize_tensor_per_tensor_affine_cpu</a> when <a class="reference external" href="https://github.com/pytorch/FBGEMM">FBGEMM</a>
is available.</p>
<p>Otherwise, it uses <a class="reference external" href="https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp#L3533">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp#L3533</a>.</p>
<blockquote>
<div><ul>
<li><p>For arm, it uses <a class="reference external" href="https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp#L3274">quantize_tensor_arm</a>. It is a template with many specializations.</p></li>
<li><p>For x86, it uses <code class="docutils literal notranslate"><span class="pre">quantize_val</span></code></p>
<ul>
<li><p>If <a class="reference external" href="https://github.com/pytorch/FBGEMM">FBGEMM</a> is available, it uses
<a class="reference external" href="https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/AffineQuantizerBase.cpp#L39">quantize_val</a></p></li>
<li><p>Otherwise, it uses <a class="reference external" href="https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/AffineQuantizerBase.cpp#L100">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/AffineQuantizerBase.cpp#L100</a></p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="n">T</span><span class="w"> </span><span class="n">quantize_val</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">scale</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">zero_point</span><span class="p">,</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// std::nearbyint results in nearest integer value according to the current</span>
<span class="w">  </span><span class="c1">// rounding mode and the default rounding mode is rounds to even in half-way</span>
<span class="w">  </span><span class="c1">// cases in most popular processor architectures like x86 and ARM. This is</span>
<span class="w">  </span><span class="c1">// typically faster than an alternatives like std::round that rounds half-way</span>
<span class="w">  </span><span class="c1">// cases away from zero, and can be consistent with SIMD implementations for</span>
<span class="w">  </span><span class="c1">// example in x86 using _mm512_cvtps_epi32 or mm512_round_ps with</span>
<span class="w">  </span><span class="c1">// _MM_FROUND_CUR_DIRECTION option that also follow the current rounding mode.</span>
<span class="w">  </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">qvalue</span><span class="p">;</span>
<span class="w">  </span><span class="k">constexpr</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">qmin</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">numeric_limits</span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">::</span><span class="n">underlying</span><span class="o">&gt;::</span><span class="n">min</span><span class="p">();</span>
<span class="w">  </span><span class="k">constexpr</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">qmax</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">numeric_limits</span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">::</span><span class="n">underlying</span><span class="o">&gt;::</span><span class="n">max</span><span class="p">();</span>
<span class="w">  </span><span class="kt">float</span><span class="w"> </span><span class="n">inv_scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0f</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">scale</span><span class="p">);</span>
<span class="w">  </span><span class="n">qvalue</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">zero_point</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Round</span><span class="p">(</span><span class="n">value</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">inv_scale</span><span class="p">));</span>
<span class="w">  </span><span class="n">qvalue</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">max</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">qvalue</span><span class="p">,</span><span class="w"> </span><span class="n">qmin</span><span class="p">);</span>
<span class="w">  </span><span class="n">qvalue</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">min</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">qvalue</span><span class="p">,</span><span class="w"> </span><span class="n">qmax</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="n">qvalue</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</div></blockquote>
<p><a class="reference external" href="https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/AffineQuantizerBase.cpp#L164">dequantize_val</a> is defined as:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">dequantize_val</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">scale</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">zero_point</span><span class="p">,</span><span class="w"> </span><span class="n">T</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">value</span><span class="p">.</span><span class="n">val_</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">zero_point</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Quantization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="quantize_per_tensor.html" class="btn btn-neutral float-right" title="torch.quantize_per_tensor" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, fangjun.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>